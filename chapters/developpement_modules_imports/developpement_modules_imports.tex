\section{Contexte du projet}

Le \gls{sinp}, déployé à La Réunion à travers la plateforme Borbonica, constitue le dispositif régional de référence pour la centralisation, 
la gestion et la diffusion des données naturalistes. Depuis sa mise en service, il a permis de structurer et de valoriser un volume important 
d’observations, issues de sources multiples et couvrant un large éventail de taxons.

La dynamique de peuplement de la base reflète toutefois les modalités historiques et opérationnelles de production des données 
naturalistes sur le territoire. Les observations actuellement intégrées concernent majoritairement les milieux terrestres et 
dulçaquicoles, qui bénéficient de réseaux d’acteurs structurés, de protocoles d’acquisition largement diffusés et d’outils de 
saisie directement compatibles avec le SINP.

À l’inverse, les données issues du milieu marin demeurent plus faiblement représentées. Les indicateurs de contenu disponibles 
en 2024 montrent que les observations relatives aux taxons marins ne constituent qu’une part marginale des données référencées. 
Cette situation ne traduit pas une absence de connaissances ou de suivis en milieu marin.
Elle résulte principalement de la complexité des chaînes d’acquisition, de structuration et de diffusion propres aux données marines, 
historiquement gérées au sein de systèmes d’information spécialisés.

Dans ce contexte, l’enjeu principal n’était pas de refondre les outils existants, mais de renforcer progressivement la représentation 
des données marines au sein du \gls{sinp}, en s’appuyant sur des sources de données déjà structurées, pérennes et reconnues 
institutionnellement. Parmi celles-ci, le système d’information \gls{quadrige}, maintenu par l’Ifremer, occupe une place centrale. 
\gls{quadrige} regroupe un volume important de données environnementales et biologiques collectées en milieu marin, issues de programmes 
de surveillance, d’observation et de recherche conduits sur le long terme.

L’ouverture récente d’une \gls{api} \gls{graphql} par l’Ifremer constitue à ce titre une évolution majeure. Elle permet une interrogation 
directe, fine et structurée des données de \gls{quadrige}, en ciblant précisément les programmes, les zones géographiques, les périodes 
temporelles et les types d’observations d’intérêt. Le fonctionnement de l’API Quadrige et les modalités d’extraction des données sont décrits 
dans la documentation officielle mise à disposition par 
l’Ifremer \href{https://quadrige-core.ifremer.fr/api/extraction/doc?doc=standard&lang=fr&name=result&type=standard}{Documentation officielle de l’API Quadrige}.
Cette évolution ouvre la voie à une exploitation plus systématique de ces données
par des acteurs institutionnels, et à leur mobilisation progressive dans le cadre du \gls{sinp}.

C’est dans ce cadre que s’inscrit le projet développé durant le stage. L’objectif n’était pas de réaliser une intégration 
directe et entièrement automatisée des données \gls{quadrige} dans la base GeoNature, mais de concevoir un module externe permettant 
d’explorer, d’extraire et de préparer ces données de manière structurée et contrôlée. Le module développé interroge 
l’\gls{api} \gls{quadrige} afin d’identifier les programmes pertinents pour le territoire réunionnais, puis permet d’en extraire les observations 
associées selon des critères définis par l’utilisateur.

Les résultats des extractions sont volontairement produits sous forme de fichiers intermédiaires,
et non intégrés directement dans la base GeoNature.
Ce choix permet de garantir la traçabilité des opérations,
de documenter précisément les paramètres d’extraction
et de laisser aux administrateurs un contrôle explicite
sur les données avant toute intégration ultérieure.
Les modalités techniques de génération, de filtrage
et de structuration de ces fichiers
sont détaillées dans la section consacrée
à la conception du module.


Cette approche intermédiaire répond à plusieurs objectifs. Elle garantit d’une part la traçabilité complète des extractions réalisées 
depuis \gls{quadrige}, en conservant une description explicite des paramètres et traitements appliqués. Elle offre d’autre part une souplesse 
d’usage, en laissant aux administrateurs et gestionnaires de données la possibilité de contrôler, d’analyser et, le cas échéant, d’adapter 
les fichiers produits avant toute intégration dans GeoNature ou Borbonica.


Ce développement s’inscrit dans la dynamique d’amélioration continue portée par la DEAL Réunion
et vise à renforcer durablement la représentation des données marines
au sein du système régional.

Afin de préserver la lisibilité du chapitre tout en fournissant
un niveau de détail suffisant sur les architectures, les flux de données
et les interfaces du module développé, les schémas fonctionnels
et les captures d’écran les plus détaillés ont été regroupés en annexe.
Le chapitre présente uniquement les éléments nécessaires à la compréhension
globale du fonctionnement du module, tandis que les figures complètes
sont accessibles en annexe pour une consultation approfondie.



\section{Périmètre fonctionnel}

Le module d’import devait couvrir l’ensemble de la chaîne d’acquisition : de la découverte des
programmes \gls{quadrige} jusqu’à la production d’un fichier structuré pour GeoNature.  
La première étape consistait à interroger l’\gls{api} en mode authentifié afin d’obtenir la liste des
programmes disponibles pour un utilisateur donné. Un filtrage automatique sur un périmètre
géographique — principalement La Réunion, mais extensible à d’autres territoires comme les Îles
Éparses — permettait d’isoler les programmes pertinents. Une interface dédiée intégrée à
GeoNature offrait ensuite la possibilité de rechercher des programmes, d’affiner l’affichage par
mots-clés et de sélectionner ceux à importer.

Le module couvre les étapes d’exploration,
de sélection et d’extraction des données,
mais ne réalise pas l’import direct en base GeoNature.
Il constitue une brique préparatoire,
positionnée en amont des mécanismes d’intégration existants.


Une fois les programmes choisis, l’utilisateur pouvait définir les filtres à appliquer aux données
(la période d'intérêt et les champs souhaités, à l’exception de la \textit{monitoring location}, automatiquement reprise à partir des programmes sélectionnés).  
Le module interrogeait alors l'api
pour récupérer les observations correspondantes.



Enfin, le module produisait un fichier CSV intermédiaire, destiné à être importé via
l’infrastructure existante de GeoNature. 
Ce fichier constitue un support de contrôle et de préparation,
distinct des archives produites lors de l’extraction des données proprement dite.

Chaque opération est historisée afin de permettre
le suivi des actions réalisées,
l’identification des erreurs éventuelles
et la consultation des extractions antérieures.
L’accès au module est restreint aux profils administrateurs.


\section{Conception technique du module d’import \gls{quadrige}}

Le module \gls{quadrige} développé durant le stage a été conçu comme un module externe à GeoNature, 
respectant l’architecture recommandée par le projet tout en prenant en compte les contraintes 
spécifiques de l’\gls{api} \gls{graphql} mise à disposition par l’Ifremer. L’objectif principal était de 
proposer une chaîne d’extraction robuste, traçable et exploitable, sans perturber le cœur 
applicatif de GeoNature ni les processus existants de gestion des données naturalistes.


\subsection{Architecture générale et découplage frontend/backend}

Le module repose sur une architecture client--serveur classique, intégrée à GeoNature via un 
\textit{blueprint} Flask côté backend et un module Angular dédié côté frontend.  
Le backend est responsable de l’ensemble des interactions avec l’\gls{api} \gls{quadrige}, incluant 
l’authentification, la construction des requêtes \gls{graphql}, le suivi des extractions, la gestion 
des fichiers produits et l’exposition de routes REST sécurisées.

Le frontend se limite volontairement au pilotage des opérations : sélection des paramètres, 
lancement des extractions et visualisation des résultats. Il n’accède jamais directement à 
l’\gls{api} \gls{quadrige} ni aux paramètres sensibles, ce qui permet de renforcer la sécurité globale du 
dispositif.

Ce découplage permet :
\begin{itemize}
  \item de centraliser les accès à l’\gls{api} \gls{quadrige} et les jetons d’authentification ;
  \item de limiter la surface d’exposition des données sensibles ;
  \item de faciliter la maintenance et les évolutions indépendantes du backend et du frontend.
\end{itemize}

\subsection{Gestion de la configuration et des paramètres sensibles}

Le module s’appuie sur un fichier de configuration dédié, chargé côté backend via le mécanisme 
standard de GeoNature (\texttt{geonature/config}). Ce fichier centralise l’ensemble des paramètres 
techniques nécessaires au fonctionnement du module, notamment :
\begin{itemize}
  \item l’URL de l’\gls{api} \gls{graphql} \gls{quadrige} ;
  \item le jeton d’authentification requis pour les appels à l’\gls{api} ;
  \item des paramètres métiers optionnels, tels que les localisations suggérées ou les champs 
  extractibles.
\end{itemize}

Une route backend permet d’exposer cette configuration au frontend de manière contrôlée, afin 
de rendre l’interface dynamique et adaptable. Certaines données métiers restent actuellement 
définies côté frontend, mais l’architecture mise en place permettrait de les centraliser 
entièrement côté backend dans une version ultérieure, sans remise en cause du fonctionnement 
global.

\subsection{Extraction des programmes \gls{quadrige}}

La première étape du processus consiste à identifier les programmes \gls{quadrige} pertinents pour un 
territoire donné. Cette extraction repose sur une requête \gls{graphql} spécifique fournie par l’\gls{api} 
\gls{quadrige}, filtrée à partir d’un préfixe de \textit{monitoring location} correspondant au périmètre 
géographique ciblé.

L’\gls{api} \gls{quadrige} retournant l’ensemble des instances associées à un programme dès lors qu’au moins 
une station correspond au critère de recherche, un traitement complémentaire est appliqué côté 
backend. Les fichiers CSV bruts générés par l’\gls{api} sont filtrés a posteriori à l’aide de la 
bibliothèque \texttt{pandas}, afin de ne conserver que les programmes effectivement localisés sur 
le territoire demandé.

Cette étape permet de produire :
\begin{itemize}
  \item un CSV brut, conservé à des fins de traçabilité et de vérification ;
  \item un CSV filtré, utilisé pour l’affichage et la sélection des programmes dans le frontend.
\end{itemize}

Les métadonnées associées à chaque extraction (localisation, horodatage, filtre utilisé) sont 
sauvegardées afin de permettre la reprise des extractions et la consultation des résultats 
antérieurs.

La Figure~\ref{fig:data-flow-program-extraction} illustre le cycle complet d’extraction des programmes
Quadrige, depuis la sélection d’un périmètre géographique par l’administrateur
jusqu’à la mise à disposition des fichiers CSV exploitables dans GeoNature.
Elle met en évidence le fonctionnement asynchrone de l’API Quadrige
et le rôle du backend GeoNature dans la transformation et le filtrage des résultats retournés.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{croped_images/data_flow_schematics/extraction_programs_flow.png}
    \caption{Flux général d’extraction des programmes Quadrige}
    \label{fig:flow-program-extraction}
\end{figure}

La version détaillée de ce schéma est fournie en annexe
(voir Annexe~\ref{ann:flow-program-extraction}).


Cette figure synthétise le mécanisme de filtrage géographique
appliqué aux résultats retournés par l’API Quadrige,
ainsi que la séparation entre données brutes
et données exploitées par l’interface GeoNature.

\subsection{Extraction des données et orchestration des traitements asynchrones}

L’extraction des données repose sur un mécanisme asynchrone propre à l’\gls{api} \gls{quadrige}. Pour chaque 
programme sélectionné, une requête \gls{graphql} est soumise afin de lancer un job d’extraction côté 
serveur Ifremer. Le module implémente un mécanisme de \textit{polling} permettant de suivre 
l’évolution de l’état de chaque extraction (PENDING, RUNNING, SUCCESS, WARNING, ERROR).

Les extractions sont gérées de manière indépendante, ce qui permet :
\begin{itemize}
  \item de paralléliser les traitements sur plusieurs programmes ;
  \item de gérer finement les échecs ou avertissements retournés par l’\gls{api} ;
  \item d’éviter un blocage global en cas d’échec partiel.
\end{itemize}

À l’issue des traitements, les fichiers ZIP générés par l’\gls{api} \gls{quadrige} sont téléchargés et stockés 
temporairement sur le serveur GeoNature. Chaque fichier est renommé selon une convention 
explicite intégrant le programme, la localisation et l’horodatage, garantissant ainsi sa 
traçabilité.

\subsection{Gestion des états, des erreurs et de la traçabilité}

Un soin particulier a été apporté à la gestion des états et des erreurs tout au long du processus 
d’extraction. Les situations suivantes sont explicitement prises en compte :

Les principaux cas d’erreur et d’avertissement
(retards, réponses partielles, absence de données)
sont détectés et signalés de manière explicite,
afin de permettre une analyse fine des résultats.


Chaque extraction génère un retour structuré indiquant son statut, les avertissements éventuels 
et les liens vers les fichiers produits. Ces informations sont transmises au frontend afin de 
permettre à l’utilisateur d’identifier rapidement les extractions exploitables et celles 
nécessitant une vérification.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{croped_images/data_flow_schematics/extraction_data_flow.png}
    \caption{Organisation générale de l’extraction des données Quadrige}
    \label{fig:flow-data-extraction}
\end{figure}

La version détaillée de ce schéma est présentée en annexe
(voir Annexe~\ref{ann:flow-data-extraction}).


Cette figure illustre la séparation fonctionnelle
entre l’identification des programmes
et l’extraction des données associées.
Cette organisation garantit la cohérence
du périmètre géographique
et sécurise le lancement des traitements.





\subsection{Interface utilisateur et pilotage des extractions}

Afin de rendre le fonctionnement du module plus concret, les figures suivantes présentent 
les principales interfaces utilisateur du module \gls{quadrige}, depuis l’exploration des programmes 
jusqu’à l’accès aux résultats d’extraction.


Le frontend Angular du module a été conçu pour accompagner l’utilisateur à travers les 
différentes étapes du processus : définition des filtres, extraction des programmes, sélection 
des données et lancement des extractions. Des contrôles de cohérence sont intégrés afin de 
prévenir les erreurs de paramétrage, notamment sur les champs obligatoires ou les périodes 
temporelles.

En fonction du périmètre géographique ciblé, l'extraction peut retourner plusieurs dizaines de programmes. 
Il a donc fallu tenir compte de leur nombre potentiellement élevé, en integrant
des mécanismes de recherche par mots-clés afin de faciliter leur exploration 
et leur sélection. Il est ainsi possible de filtrer dynamiquement la liste des programmes extraits 
en saisissant un terme dans la barre de recherche, permettant de cibler rapidement un programme 
par son nom ou ses métadonnées associées.


La Figure~\ref{fig:ui-program-list} présente l’interface du module
à l’issue d’une extraction réussie des programmes Quadrige
pour un périmètre géographique donné.
Elle correspond à l’état stable du système
une fois le traitement asynchrone d’extraction terminé
et les fichiers CSV filtrés générés côté backend.



\begin{figure}[H]
    \centering
    \includegraphics[width = 0.9\textwidth]{croped_images/quadrige/extracted_programs.png}
    \caption{Interface de consultation et de sélection des programmes \gls{quadrige}
      après extraction pour le territoire de La Réunion,
      incluant les métadonnées des programmes
      et les outils de recherche et de sélection}
    \label{fig:ui-program-list}
\end{figure}

L’interface présentée met en évidence plusieurs informations clés
issues directement du traitement d’extraction des programmes.
Le bandeau supérieur rappelle le périmètre géographique utilisé,
via le code de \textit{monitoring location} associé à La Réunion,
ainsi que le nombre total de programmes extraits pour ce territoire.


Les programmes sont en outre explicitement sélectionnables,
via des cases à cocher,
ce qui permet à l’utilisateur de constituer
un sous-ensemble maîtrisé de programmes.
Cette sélection conditionne directement
la phase suivante d’extraction des données,
sans nécessiter de nouvelle saisie du périmètre géographique.


Cette interface joue ainsi un rôle central dans la chaîne d’extraction.
Elle constitue le point de transition
entre la phase d’identification des programmes
et celle de l’extraction des données associées.
En matérialisant visuellement
les résultats du filtrage géographique
et en permettant une sélection fine,
elle sécurise les traitements ultérieurs
et renforce la maîtrise fonctionnelle de l’utilisateur
avant le lancement d’extractions potentiellement coûteuses.


Cette logique de recherche est également appliquée lors des phases de paramétrage. 
Dans les interfaces de filtrage, l’utilisateur peut rechercher les champs disponibles à l’extraction 
en saisissant directement du texte dans le champ dédié, ce qui facilite la sélection lorsque la 
liste des champs est étendue. De la même manière, les localisations suggérées lors de la définition 
des filtres peuvent être recherchées par saisie textuelle, améliorant l’ergonomie et limitant les 
erreurs de sélection.


La Figure~\ref{fig:ui-program-filter} présente la fenêtre de paramétrage
utilisée pour définir les filtres appliqués
lors de l’extraction des programmes Quadrige.
Cette interface constitue le point d’entrée
du processus d’extraction,
en permettant à l’utilisateur de spécifier explicitement
le périmètre géographique ciblé
avant toute interrogation de l’API.



\begin{figure}[H]
    \centering
    \includegraphics[width = 0.9\textwidth]{croped_images/quadrige/program_filter.png}
    \caption{Fenêtre de définition des filtres d’extraction des programmes \gls{quadrige},
      incluant la sélection de la \textit{monitoring location}}

    \label{fig:ui-program-filter}
\end{figure}


Le champ central de cette interface correspond
au code de \textit{monitoring location},
paramètre indispensable à l’identification
des programmes Quadrige.
La localisation peut être sélectionnée
de deux manières complémentaires :
soit en choisissant une valeur
dans le menu déroulant proposé,
soit en saisissant directement le code attendu.


Ce champ remplit également une fonction de recherche dynamique.
La liste des localisations proposées
est filtrée en temps réel
à partir de la saisie de l’utilisateur.
Par exemple, la saisie partielle du terme « May »
restreint immédiatement la liste
au seul résultat correspondant à Mayotte,
associé au code \texttt{145-}.
Ce comportement facilite la navigation
dans des listes potentiellement étendues
et limite les erreurs de saisie.


Des règles de validation strictes sont appliquées
avant l’acceptation du filtre.
La définition d’une \textit{monitoring location}
est obligatoire :
le filtre ne peut pas être validé
tant qu’aucune localisation n’est renseignée.
De même, le nom associé au filtre
doit comporter un minimum de trois caractères.


Cette contrainte sur la longueur minimale du nom du filtre
résulte d’un retour d’expérience lors des phases de test.
Dans une version initiale du module,
les filtres portant un nom trop court
étaient acceptés côté interface,
mais provoquaient un échec silencieux
de la requête côté API Quadrige.
L’identification de cette source d’erreur
a conduit à l’intégration
d’une règle de validation explicite,
permettant de prévenir ce type de dysfonctionnement
dès la saisie utilisateur.


L’ensemble de ces mécanismes contribue
à fiabiliser la phase d’extraction des programmes,
en empêchant l’envoi de requêtes incomplètes ou invalides
vers l’API Quadrige.
L’interface joue ainsi un rôle actif
dans la prévention des erreurs,
en traduisant les contraintes techniques de l’API
en règles de saisie compréhensibles
et immédiatement vérifiables par l’utilisateur.



Les résultats des extractions sont présentés sous forme de liens téléchargeables, accompagnés 
d’indicateurs visuels signalant les éventuels avertissements. Cette approche permet à 
l’administrateur de conserver une maîtrise complète sur les données produites avant toute 
intégration dans GeoNature ou dans des plateformes partenaires telles que Borbonica.




La Figure~\ref{fig:ui-extraction-results} présente l’interface de consultation
des résultats produits par le module Quadrige
à l’issue des différentes phases d’extraction.
Cette vue constitue un point central du dispositif,
en offrant à l’utilisateur un accès direct
à l’ensemble des fichiers générés,
avant toute intégration ultérieure dans GeoNature.

\begin{figure}[H]
    \centering
    \includegraphics[width = 0.9\textwidth]{croped_images/quadrige/extracted_data.png}
    \caption{Interface de consultation des fichiers produits
      lors des extractions de programmes et de données \gls{quadrige}}
    \label{fig:ui-extraction-results}
\end{figure}

La première section de l’interface,
intitulée « Programmes extraits »,
regroupe les fichiers produits
lors de la phase d’extraction des programmes.
Deux fichiers CSV distincts y sont systématiquement proposés.


Le premier correspond au CSV brut
retourné directement par l’API Quadrige.
Il contient l’ensemble des instances
associées aux programmes identifiés,
y compris celles localisées en dehors
du périmètre géographique ciblé.
Ce fichier est conservé à des fins de traçabilité
et permet de documenter fidèlement
la réponse initiale fournie par l’API.


Le second fichier correspond au CSV filtré,
généré après traitement côté backend GeoNature.
Ce filtrage permet d’éliminer les instances
dont le code de \textit{monitoring location}
ne correspond pas au territoire sélectionné par l’utilisateur.
Il s’agit du fichier de référence
utilisé par l’interface
pour l’affichage et la sélection des programmes.


La seconde section de l’interface,
intitulée « Données extraites »,
présente les résultats de la phase d’extraction
des observations associées aux programmes sélectionnés.
Chaque programme ayant fait l’objet
d’une extraction aboutie
donne lieu à la production d’une archive compressée distincte.


Chaque archive ZIP correspond
à un programme Quadrige unique.
Ce découpage volontaire permet
de conserver une séparation claire
entre les jeux de données,
facilitant leur analyse, leur vérification
et leur exploitation ultérieure.
Il permet également de gérer finement
les erreurs ou avertissements,
sans compromettre l’ensemble des extractions.


Les fichiers générés sont nommés
selon une convention explicite et homogène.
Le nom de chaque archive intègre
le code de la \textit{monitoring location},
la date et l’heure de l’extraction,
ainsi que l’identifiant du programme concerné.
Cette convention permet de retrouver rapidement
le contexte de production d’un fichier,
d’en faciliter le tri
et de garantir une traçabilité complète des traitements réalisés.


Cette interface de restitution illustre
la philosophie générale du module Quadrige :
les données ne sont pas intégrées automatiquement,
mais mises à disposition sous une forme structurée,
documentée et contrôlable.
L’administrateur peut ainsi consulter les résultats,
identifier les extractions exploitables
et conserver une maîtrise complète
sur les données produites
avant toute intégration dans GeoNature ou Borbonica.



Une interface spécifique permet également de définir les filtres d’extraction des données 
associées aux programmes sélectionnés (périodes temporelles, localisations, champs à extraire). 
À des fins de lisibilité, cette interface est présentée en annexe (voir Figure~\ref{fig:data-filter}).




\subsection{Synthèse}

L’architecture retenue pour le module \gls{quadrige} répond aux contraintes spécifiques de l’\gls{api} 
Ifremer tout en respectant les principes de modularité, de sécurité et de traçabilité propres à 
GeoNature. Elle constitue une base technique solide pour des évolutions futures, notamment vers 
une automatisation partielle des imports, tout en conservant un contrôle humain sur les données 
produites.

\section{Contraintes, dépendances et livrables du module \gls{quadrige}}

La réussite du projet dépendait principalement de l’accessibilité de l’\gls{api} \gls{quadrige}, de la
stabilité de ses services, et de la disponibilité d’une documentation actualisée. L’intégration dans
GeoNature nécessitait également de respecter la structure modulaire du noyau applicatif et les
contraintes du modèle \gls{sinp}.

Les livrables attendus comprenaient le code source du module, un fichier de configuration, une
documentation à destination des administrateurs (installation, configuration, maintenance) ainsi
qu’un guide d’utilisation orienté métier.

Plusieurs évolutions ont été envisagées : automatisation des imports périodiques, gestion des
imports incrémentaux, intégration à la gestion des droits de GeoNature et, à plus long terme,
publication du module dans le catalogue officiel des extensions GeoNature.



\section{Extension du module api2GN pour l’intégration des données Pl@ntNet}

\subsection{Rôle du module \gls{api2gn} dans l’écosystème GeoNature}

Le module \texttt{\gls{api2gn}} constitue une brique transversale au sein de l’écosystème GeoNature. 
Il se distingue des modules d’import classiques, généralement fondés sur des fichiers intermédiaires produits manuellement, 
en permettant l’intégration directe et automatisée de données issues de sources externes.


Ces sources sont exposées via des services web, tels que des \gls{apirest}, des flux JSON structurés ou des services WFS. 
Le module a été conçu pour répondre à des besoins d’automatisation et de récurrence, tout en respectant les contraintes 
du modèle de données du \gls{sinp} et de la table \textit{Synthèse} de GeoNature.

Le code source du module api2GN est disponible publiquement sur GitHub \href{https://github.com/PnX-SI/api2GN}{Dépôt officiel api2GN}.

Son fonctionnement repose sur une séparation claire entre deux niveaux. 
Le premier correspond au cœur du moteur d’import, chargé de l’orchestration des appels aux \gls{api}, 
de la gestion des erreurs, de l’historisation des opérations et de l’insertion des données en base. 
Le second niveau est constitué de parseurs spécialisés, responsables de la transformation des données sources 
vers le modèle attendu par GeoNature.

Chaque parseur implémente ainsi une logique métier propre à une source donnée, tout en s’appuyant sur un socle commun. 
Ce socle prend en charge la gestion des géométries, la résolution des nomenclatures, la validation des mappings, 
l’historisation des imports et l’intégration aux mécanismes de sécurité de GeoNature.

Dans ce contexte, le module \texttt{\gls{api2gn}} apparaît particulièrement adapté à l’intégration de données issues 
de plateformes participatives ou ouvertes, telles que Pl@ntNet. 
Ces plateformes se caractérisent par des volumes importants, une fréquence de mise à jour élevée 
et des modalités d’accès très différentes de celles des bases institutionnelles comme \gls{quadrige}.

\subsection{Objectifs et positionnement du parser Pl@ntNet}

On peut retrouver la documentation du fonctionnement de l'API Pl@ntnet utilisée à cette adresse :  
\href{https://my-api.plantnet.org/\#/DarwinCore/postV3DwcOccurrenceSearch}{Documentation de l’API Pl@ntNet}

Le développement du parser Pl@ntNet s’inscrit dans une logique complémentaire à celle du module \gls{quadrige}. 
Alors que l’import \gls{quadrige} vise des données institutionnelles, produites dans un cadre scientifique structuré 
et contrôlé, les données issues de Pl@ntNet relèvent d’un contexte participatif.

Ce type de données se caractérise par une production massive et continue d’observations, 
une grande hétérogénéité des contributeurs et une variabilité importante de la qualité des identifications. 
La fiabilité des données repose en grande partie sur des mécanismes automatisés de reconnaissance et de validation.

Dans ce contexte, l’objectif du parser Pl@ntNet n’était pas de proposer une intégration exhaustive 
de l’ensemble des observations disponibles. 
L’enjeu consistait plutôt à mettre en place une chaîne d’import automatisée mais maîtrisée, 
capable de fonctionner régulièrement, tout en conservant un niveau élevé d’exigence sur la qualité des données intégrées.

Le parser a ainsi été conçu comme un outil configurable et sélectif. 
Il permet d’appliquer des filtres spatiaux, temporels et taxonomiques, 
et impose un contrôle strict de la compatibilité des données avec le référentiel \gls{taxref}.

Cette approche illustre une stratégie d’intégration différente de celle retenue pour les données \gls{quadrige}. 
Elle repose sur une automatisation plus poussée, compensée par des mécanismes renforcés de validation et de traçabilité.

\subsection{Architecture technique et principes de conception}

Sur le plan technique, le parser Pl@ntNet repose sur l’architecture standard des parseurs du module \texttt{\gls{api2gn}}. 
Il hérite directement de la classe \texttt{JSONParser}, elle-même dérivée de la classe générique \texttt{Parser}. 
Cette hiérarchie permet de mutualiser l’ensemble des fonctionnalités communes aux sources de données exposées sous forme JSON.

Le parser conserve toutefois la responsabilité de la logique métier spécifique à Pl@ntNet. 
Le déroulement général d’une exécution suit une séquence clairement définie. 
Après l’initialisation du parser et le chargement de la configuration, 
les requêtes vers l’\gls{api} Pl@ntNet sont construites. 
Les observations sont ensuite récupérées de manière paginée, puis transformées et normalisées. 
Une phase de résolution taxonomique est alors appliquée, avant l’insertion des observations valides 
dans la table \textit{Synthèse} de GeoNature.

Un principe fondamental a guidé la conception du parser. 
Aucune logique métier n’est codée en dur dans le code Python. 
Les paramètres d’appel à l’\gls{api}, les filtres appliqués, le mapping des champs 
et les règles de validation sont intégralement définis dans la configuration.

Ce choix favorise la maintenabilité du module et facilite son adaptation à d’autres contextes territoriaux, 
sans remise en cause de l’architecture existante.

\subsection{Configuration dynamique et pilotage par fichier TOML}

L’un des apports majeurs du travail réalisé concerne la gestion de la configuration du parser Pl@ntNet. 
Celui-ci s’appuie sur un fichier \texttt{TOML}, chargé via le mécanisme standard de configuration de GeoNature.

Ce fichier permet de définir de manière centralisée les paramètres d’accès à l’\gls{api} Pl@ntNet, 
les filtres taxonomiques éventuels, les bornes temporelles d’extraction et l’emprise géographique ciblée, 
définie sous la forme d’un polygone GeoJSON. 
Le mapping entre les champs fournis par Pl@ntNet et ceux de la table \textit{Synthèse} 
est également entièrement décrit dans ce fichier.



Le parser implémente un mécanisme de configuration par défaut.

En l’absence de fichier \texttt{api2gn\_config.toml}, ou si celui-ci est incomplet, 
un jeu de valeurs par défaut intégrées au code est utilisé.
Ce comportement permet de garantir le fonctionnement du module en environnement de test 
et de limiter les risques de blocage en production, tout en signalant explicitement 
les incohérences via les logs.

Une route API dédiée permet de consulter la configuration effective chargée par GeoNature.
Cette route applique une validation métier non bloquante, signalant les incohérences potentielles (dates inversées, géométrie invalide, mapping incorrect) sans empêcher l’exécution du parser.
Ce choix vise à favoriser la transparence et l’autonomie des administrateurs, tout en conservant un haut niveau de robustesse.

Le parser Pl@ntNet met également en œuvre un mécanisme d’auto-création des métadonnées nécessaires à l’intégration dans GeoNature.
Lors de la première exécution, le parser vérifie l’existence de la source, du cadre d’acquisition et du jeu de données associés à Pl@ntNet.
En l’absence de ces éléments, ils sont automatiquement créés en base, dans le respect du modèle GeoNature.
Ce mécanisme permet de rendre le parser autonome et de limiter les opérations manuelles d’initialisation lors du déploiement.

La structuration des données repose sur le standard Darwin Core, tel que défini par le \href{https://www.gbif.org/fr/darwin-core}{GBIF}.


Cette stratégie renforce la robustesse du module, notamment en phase de test ou de développement, 
tout en incitant à une configuration explicite en environnement de production. 
Elle ouvre également la voie à une future interface d’administration graphique dédiée 
au pilotage de ces paramètres.

La Figure~\ref{fig:data-flow-GeoNature-Plantnet} synthétise le déroulement général
du flux d’import des données Pl@ntNet,
depuis l’interrogation de l’API jusqu’à l’insertion des observations validées
dans la table \textit{Synthèse} de GeoNature.


\begin{figure}[H]
    \centering
    \includegraphics[width = 0.9\textwidth]{croped_images/data_flow_schematics/plantnet_data_flow.png}
    \caption{Schéma fonctionnel des échanges de données entre Géonature et Pl@ntnet}
    \label{fig:data-flow-GeoNature-Plantnet}
\end{figure}

La version détaillée de ce schéma est fournie en annexe
(voir Annexe~\ref{ann:data-flow-GeoNature-Plantnet}).
Il illustre le fonctionnement global du parser Pl@ntNet
et met en évidence le rôle central joué par la configuration
dans le pilotage de l’import.

À la différence du module \gls{quadrige},
l’import Pl@ntNet repose entièrement sur un fichier de configuration externe.

L’administrateur GeoNature définit les paramètres de l’extraction
dans un fichier \texttt{api2gn\_config.toml},
qui doit être placé dans le répertoire \texttt{geonature/config}
avant le redémarrage de l’application.
Ce mécanisme conditionne l’ensemble du comportement du parser :
périmètre géographique, période temporelle,
liste d’espèces, volume maximal de données
et règles de validation taxonomique.

Ce choix d’architecture permet une extraction entièrement personnalisée,
reproductible et traçable,
au prix d’une intervention plus technique de l’administrateur.
Il constitue un compromis assumé,
adapté à un fonctionnement automatisé et périodique,
et ouvre la voie à une évolution future vers une interface graphique dédiée,
évoquée dans le chapitre consacré aux perspectives.


Le schéma met en évidence l’existence d’un mécanisme de limitation
et de pagination des résultats retournés par l’API Pl@ntNet,
sans en détailler le fonctionnement interne.
Cette abstraction volontaire permet de conserver
une lecture fonctionnelle du flux,
centrée sur la construction de la requête
et la réception des occurrences.

Concrètement, le contrôle du volume de données importées
repose sur une pagination basée sur un couple
\texttt{limit}/\texttt{offset},
imposé par l’API Pl@ntNet et exploité par le parser.
La valeur de la limite est définie
par le paramètre \texttt{plantnet\_max\_data}
du fichier de configuration.
Elle correspond au nombre maximal d’occurrences
récupérées lors d’un appel à l’API
et conditionne la taille des blocs traités successivement.

Après chaque appel,
le parser incrémente automatiquement l’offset
afin de solliciter la page suivante de résultats.
L’itération se poursuit
jusqu’à ce que l’API retourne un nombre d’occurrences
inférieur à la limite demandée,
ce qui marque la fin des données disponibles
pour le périmètre et les filtres définis.

Ce mécanisme permet de maîtriser précisément
le volume total d’observations traitées,
tout en limitant la charge réseau et mémoire.
Un seuil de sécurité supplémentaire est implémenté côté parser
afin d’interrompre l’extraction
si le nombre de pages devient excessif,
prévenant ainsi toute dérive liée à une configuration inadaptée
ou à un comportement inattendu de l’API distante.



\subsection{Résolution taxonomique et contrôle de la qualité des données}

La résolution taxonomique constitue l’enjeu central de l’intégration des données Pl@ntNet dans GeoNature. 
Afin de garantir la cohérence avec le référentiel national TAXREF et les exigences du \gls{sinp}, 
le parser met en œuvre une chaîne de validation stricte.

Pour chaque observation, le nom scientifique fourni par l’\gls{api} Pl@ntNet est d’abord normalisé. 
Les mentions infra-spécifiques et les annotations non standard sont supprimées afin d’améliorer 
les chances de correspondance avec le référentiel.

Le parser tente ensuite de résoudre le \texttt{cd\_nom} en interrogeant en priorité le référentiel TAXREF local. 
En cas d’échec, le service \gls{taxrefld} du \gls{mnhn} est sollicité. 
Le \texttt{cd\_nom} obtenu est enfin vérifié dans la base locale de GeoNature.

Afin d’optimiser les performances lors des imports volumineux, un cache mémoire des résolutions taxonomiques est utilisé.
Ce cache permet d’éviter des requêtes répétées vers la base TAXREF locale ou les services \gls{taxrefld} pour un même nom scientifique, 
réduisant significativement le temps de traitement.


Par défaut, le parser fonctionne en mode strict. 
Toute observation pour laquelle aucun \texttt{cd\_nom} valide ne peut être résolu est rejetée. 
Ces rejets sont systématiquement journalisés, ce qui permet d’identifier les taxons problématiques 
et d’évaluer la qualité globale des données importées.

La Figure~\ref{fig:process-validation-TAXREF} détaille la procédure de résolution
et de validation du nom scientifique appliquée à chaque observation Pl@ntNet.


\begin{figure}[H]
    \centering
    \includegraphics[width = 0.9\textwidth]{croped_images/data_flow_schematics/validation_taxref.png}
    \caption{Schema de la procédure de validation du nom scientifique retourné par Pl@ntnet}
    \label{fig:process-validation-TAXREF}
\end{figure}

La version détaillée de ce schéma est fournie en annexe
(voir Annexe~\ref{ann:process-validation-TAXREF}).

Cette chaîne de validation illustre le rôle clé du référentiel TAXREF
dans la sécurisation des imports.
Le recours prioritaire au référentiel local,
complété par l’appel au service TAXREF-LD,
permet de concilier performance et exhaustivité,
tout en garantissant la cohérence avec le modèle du \gls{sinp}.



\subsection{Traçabilité, sécurité et automatisation des imports}

Le parser Pl@ntNet s’intègre pleinement aux mécanismes de traçabilité fournis par le module \texttt{\gls{api2gn}}. 
Chaque exécution est historisée dans une table dédiée. 
Cette historisation permet de conserver la date du dernier import, 
le nombre d’observations intégrées et le volume total de données importées.

Un mode \textit{dry-run} est systématiquement disponible. 
Il permet de simuler un import sans insertion en base. 
Ce mode constitue un outil essentiel pour tester une nouvelle configuration, 
évaluer l’impact d’un changement de périmètre ou analyser la qualité des données 
avant une intégration effective.

Le module intègre un mécanisme d’automatisation basé sur Celery.
Les parseurs peuvent être configurés avec une fréquence d’exécution stockée en base.
Un processus périodique vérifie les dates de dernier import et déclenche automatiquement les imports lorsque la 
fréquence définie est atteinte. Il devient ainsi possible d’envisager des imports périodiques automatisés, 
tout en conservant un contrôle fin sur leur déclenchement et leur suivi.

\subsection{Apports du travail réalisé}

Le développement du parser Pl@ntNet a permis de démontrer la capacité de GeoNature 
à intégrer des données participatives de manière automatisée, 
tout en respectant les exigences du \gls{sinp} en matière de qualité, de traçabilité 
et de cohérence taxonomique.

Au-delà du cas spécifique de Pl@ntNet, ce travail met en évidence la pertinence 
de l’architecture \texttt{\gls{api2gn}} pour l’intégration de sources de données hétérogènes. 
Il souligne également l’intérêt d’une configuration externalisée 
pour favoriser la réutilisation du module et son adaptation à différents contextes.

Le parser Pl@ntNet constitue ainsi un prototype fonctionnel et robuste. 
Il ouvre la voie à l’intégration d’autres sources de données ouvertes 
et contribue à l’enrichissement progressif de la base GeoNature, 
notamment pour les taxons terrestres peu couverts par les réseaux institutionnels traditionnels.

Le code source du parser Pl@ntNet et des modules développés durant le stage est versionné et documenté, 
afin d’en faciliter la reprise et l’évolution, les références étant fournies en Webographie.
