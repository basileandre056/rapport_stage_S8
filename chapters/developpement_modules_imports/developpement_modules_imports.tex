\section{Contexte du projet}

Le \gls{sinp}, déployé à La Réunion à travers la plateforme Borbonica, constitue le dispositif régional de référence pour la centralisation, 
la gestion et la diffusion des données naturalistes. Depuis sa mise en service, il a permis de structurer et de valoriser un volume important 
d’observations, issues de sources multiples et couvrant un large éventail de taxons.

La dynamique de peuplement de la base reflète toutefois les modalités historiques et opérationnelles de production des données 
naturalistes sur le territoire. Les observations actuellement intégrées concernent majoritairement les milieux terrestres et 
dulçaquicoles, qui bénéficient de réseaux d’acteurs structurés, de protocoles d’acquisition largement diffusés et d’outils de 
saisie directement compatibles avec le SINP.

À l’inverse, les données issues du milieu marin demeurent plus faiblement représentées. Les indicateurs de contenu disponibles 
en 2024 montrent que les observations relatives aux taxons marins ne constituent qu’une part marginale des données référencées. 
Cette situation ne traduit pas une absence de connaissances ou de suivis en milieu marin, mais résulte principalement de la 
complexité des chaînes d’acquisition, de structuration et de diffusion propres aux données marines, historiquement gérées au sein 
de systèmes d’information spécialisés.

Dans ce contexte, l’enjeu principal n’était pas de refondre les outils existants, mais de renforcer progressivement la représentation 
des données marines au sein du \gls{sinp}, en s’appuyant sur des sources de données déjà structurées, pérennes et reconnues 
institutionnellement. Parmi celles-ci, le système d’information \gls{quadrige}, maintenu par l’Ifremer, occupe une place centrale. 
\gls{quadrige} regroupe un volume important de données environnementales et biologiques collectées en milieu marin, issues de programmes 
de surveillance, d’observation et de recherche conduits sur le long terme.

L’ouverture récente d’une \gls{api} \gls{graphql} par l’Ifremer constitue à ce titre une évolution majeure. Elle permet une interrogation 
directe, fine et structurée des données de \gls{quadrige}, en ciblant précisément les programmes, les zones géographiques, les périodes 
temporelles et les types d’observations d’intérêt. Le fonctionnement de l’API Quadrige et les modalités d’extraction des données sont décrits 
dans la documentation officielle mise à disposition par 
l’Ifremer \href{https://quadrige-core.ifremer.fr/api/extraction/doc?doc=standard&lang=fr&name=result&type=standard}{Documentation officielle de l’API Quadrige}.


Cette évolution ouvre la voie à une exploitation plus systématique de ces données 
par des acteurs institutionnels tels que la DEAL, et à leur mobilisation progressive dans le cadre du \gls{sinp}.

C’est dans ce cadre que s’inscrit le projet développé durant le stage. L’objectif n’était pas de réaliser une intégration 
directe et entièrement automatisée des données \gls{quadrige} dans la base GeoNature, mais de concevoir un module externe permettant 
d’explorer, d’extraire et de préparer ces données de manière structurée, traçable et reproductible. Le module développé interroge 
l’\gls{api} \gls{quadrige} afin d’identifier les programmes pertinents pour le territoire réunionnais, puis permet d’en extraire les observations 
associées selon des critères définis par l’utilisateur.
Les résultats de ces extractions ne prennent pas la forme de fichiers immédiatement importables dans GeoNature. 
Pour chaque programme sélectionné, le module génère une archive compressée contenant plusieurs éléments complémentaires : 
un fichier \gls{csv} brut correspondant aux données extraites, un fichier \gls{json} décrivant précisément les filtres appliqués lors 
de l’appel à l’\gls{api}, ainsi qu’un fichier README documentant le déroulement de l’export et les éventuelles anomalies rencontrées. 
Le module conserve également un accès aux trois derniers exports réalisés afin d’en faciliter la consultation et la réutilisation.

En complément de ces archives, le module expose également des liens directs vers les fichiers CSV générés lors de la phase d’extraction 
des programmes. Deux niveaux de fichiers sont distingués :
– un CSV brut issu directement de la réponse de l’\gls{api} \gls{quadrige}, contenant l’ensemble des instances des programmes dont au moins une 
occurrence est localisée sur le territoire ciblé ;
– un CSV filtré géographiquement, obtenu par un traitement a posteriori à l’aide de la bibliothèque pandas, permettant d’exclure 
les instances dont le code de monitoring location ne correspond pas au territoire d’intérêt (par exemple, codes ne commençant pas 
par le préfixe « 126- » pour La Réunion).

Ce filtrage complémentaire est nécessaire car, lors de l’extraction des programmes, la requête adressée à l’\gls{api} \gls{quadrige} sélectionne 
l’ensemble des programmes possédant au moins une instance sur le territoire demandé, mais retourne également les autres instances associées 
à ces programmes, y compris celles situées hors du périmètre géographique ciblé.

Cette approche intermédiaire répond à plusieurs objectifs. Elle garantit d’une part la traçabilité complète des extractions réalisées 
depuis \gls{quadrige}, en conservant une description explicite des paramètres et traitements appliqués. Elle offre d’autre part une souplesse 
d’usage, en laissant aux administrateurs et gestionnaires de données la possibilité de contrôler, d’analyser et, le cas échéant, d’adapter 
les fichiers produits avant toute intégration dans GeoNature ou Borbonica. Le module s’inscrit ainsi comme une brique préparatoire, destinée 
à sécuriser et à faciliter l’intégration future des données marines dans le \gls{sinp}.

Ce développement s’inscrit pleinement dans la dynamique d’amélioration continue portée par la DEAL Réunion et ses partenaires. 
Il vise à renforcer progressivement la place des données issues du milieu marin au sein du système régional, tout en respectant 
les contraintes techniques, méthodologiques et organisationnelles propres aux outils existants.



\section{Périmètre fonctionnel}

Le module d’import devait couvrir l’ensemble de la chaîne d’acquisition : de la découverte des
programmes \gls{quadrige} jusqu’à la production d’un fichier structuré pour GeoNature.  
La première étape consistait à interroger l’\gls{api} en mode authentifié afin d’obtenir la liste des
programmes disponibles pour un utilisateur donné. Un filtrage automatique sur un périmètre
géographique — principalement La Réunion, mais extensible à d’autres territoires comme les Îles
Éparses — permettait d’isoler les programmes pertinents. Une interface dédiée intégrée à
GeoNature offrait ensuite la possibilité de rechercher des programmes, d’affiner l’affichage par
mots-clés et de sélectionner ceux à importer.

Une fois les programmes choisis, l’utilisateur pouvait définir les filtres à appliquer aux données
(la période d'intérêt, les champs souhaités, ou encore la reprise des stations déjà extraites).  
Le module interrogeait alors l'api
pour récupérer les observations correspondantes 
Seuls les champs utiles au modèle \gls{sinp} étaient extraits : identifiants des programmes et stations,
localisation géographique, taxon observé, date, ainsi que les métadonnées essentielles (auteur,
organisme, méthode d’acquisition, etc.). Une transformation était appliquée pour obtenir une
structure compatible avec les mécanismes d’import de GeoNature.

Enfin, le module produisait un fichier CSV intermédiaire, destiné à être importé via
l’infrastructure existante de GeoNature. Chaque opération d’import était consignée dans un
historique affiché dans un second onglet, permettant de suivre les actions réalisées, leur date,
leur statut et les éventuelles erreurs rencontrées. L’ensemble du module était réservé aux
administrateurs, conformément aux pratiques habituelles de contrôle des imports dans GeoNature.

\section{Conception technique du module d’import \gls{quadrige}}

Le module \gls{quadrige} développé durant le stage a été conçu comme un module externe à GeoNature, 
respectant l’architecture recommandée par le projet tout en prenant en compte les contraintes 
spécifiques de l’\gls{api} \gls{graphql} mise à disposition par l’Ifremer. L’objectif principal était de 
proposer une chaîne d’extraction robuste, traçable et exploitable, sans perturber le cœur 
applicatif de GeoNature ni les processus existants de gestion des données naturalistes.

\subsection{Architecture générale et découplage frontend/backend}

Le module repose sur une architecture client--serveur classique, intégrée à GeoNature via un 
\textit{blueprint} Flask côté backend et un module Angular dédié côté frontend.  
Le backend est responsable de l’ensemble des interactions avec l’\gls{api} \gls{quadrige}, incluant 
l’authentification, la construction des requêtes \gls{graphql}, le suivi des extractions, la gestion 
des fichiers produits et l’exposition de routes REST sécurisées.

Le frontend se limite volontairement au pilotage des opérations : sélection des paramètres, 
lancement des extractions et visualisation des résultats. Il n’accède jamais directement à 
l’\gls{api} \gls{quadrige} ni aux paramètres sensibles, ce qui permet de renforcer la sécurité globale du 
dispositif.

Ce découplage permet :
\begin{itemize}
  \item de centraliser les accès à l’\gls{api} \gls{quadrige} et les jetons d’authentification ;
  \item de limiter la surface d’exposition des données sensibles ;
  \item de faciliter la maintenance et les évolutions indépendantes du backend et du frontend.
\end{itemize}

\subsection{Gestion de la configuration et des paramètres sensibles}

Le module s’appuie sur un fichier de configuration dédié, chargé côté backend via le mécanisme 
standard de GeoNature (\texttt{geonature/config}). Ce fichier centralise l’ensemble des paramètres 
techniques nécessaires au fonctionnement du module, notamment :
\begin{itemize}
  \item l’URL de l’\gls{api} \gls{graphql} \gls{quadrige} ;
  \item le jeton d’authentification requis pour les appels à l’\gls{api} ;
  \item des paramètres métiers optionnels, tels que les localisations suggérées ou les champs 
  extractibles.
\end{itemize}

Une route backend permet d’exposer cette configuration au frontend de manière contrôlée, afin 
de rendre l’interface dynamique et adaptable. Certaines données métiers restent actuellement 
définies côté frontend, mais l’architecture mise en place permettrait de les centraliser 
entièrement côté backend dans une version ultérieure, sans remise en cause du fonctionnement 
global.

\subsection{Extraction des programmes \gls{quadrige}}

La première étape du processus consiste à identifier les programmes \gls{quadrige} pertinents pour un 
territoire donné. Cette extraction repose sur une requête \gls{graphql} spécifique fournie par l’\gls{api} 
\gls{quadrige}, filtrée à partir d’un préfixe de \textit{monitoring location} correspondant au périmètre 
géographique ciblé.

L’\gls{api} \gls{quadrige} retournant l’ensemble des instances associées à un programme dès lors qu’au moins 
une station correspond au critère de recherche, un traitement complémentaire est appliqué côté 
backend. Les fichiers CSV bruts générés par l’\gls{api} sont filtrés a posteriori à l’aide de la 
bibliothèque \texttt{pandas}, afin de ne conserver que les programmes effectivement localisés sur 
le territoire demandé.

Cette étape permet de produire :
\begin{itemize}
  \item un CSV brut, conservé à des fins de traçabilité et de vérification ;
  \item un CSV filtré, utilisé pour l’affichage et la sélection des programmes dans le frontend.
\end{itemize}

Les métadonnées associées à chaque extraction (localisation, horodatage, filtre utilisé) sont 
sauvegardées afin de permettre la reprise des extractions et la consultation des résultats 
antérieurs.

\subsection{Extraction des données et gestion asynchrone des traitements}

L’extraction des données repose sur un mécanisme asynchrone propre à l’\gls{api} \gls{quadrige}. Pour chaque 
programme sélectionné, une requête \gls{graphql} est soumise afin de lancer un job d’extraction côté 
serveur Ifremer. Le module implémente un mécanisme de \textit{polling} permettant de suivre 
l’évolution de l’état de chaque extraction (PENDING, RUNNING, SUCCESS, WARNING, ERROR).

Les extractions sont gérées de manière indépendante, ce qui permet :
\begin{itemize}
  \item de paralléliser les traitements sur plusieurs programmes ;
  \item de gérer finement les échecs ou avertissements retournés par l’\gls{api} ;
  \item d’éviter un blocage global en cas d’échec partiel.
\end{itemize}

À l’issue des traitements, les fichiers ZIP générés par l’\gls{api} \gls{quadrige} sont téléchargés et stockés 
temporairement sur le serveur GeoNature. Chaque fichier est renommé selon une convention 
explicite intégrant le programme, la localisation et l’horodatage, garantissant ainsi sa 
traçabilité.

\subsection{Gestion des états, des erreurs et de la traçabilité}

Un soin particulier a été apporté à la gestion des états et des erreurs tout au long du processus 
d’extraction. Les situations suivantes sont explicitement prises en compte :
\begin{itemize}
  \item erreurs de communication avec l’\gls{api} \gls{graphql} ;
  \item délais excessifs lors des extractions longues ;
  \item incohérences ou réponses partielles retournées par l’\gls{api} ;
  \item extractions aboutissant à des avertissements ou à l’absence de données.
\end{itemize}

Chaque extraction génère un retour structuré indiquant son statut, les avertissements éventuels 
et les liens vers les fichiers produits. Ces informations sont transmises au frontend afin de 
permettre à l’utilisateur d’identifier rapidement les extractions exploitables et celles 
nécessitant une vérification.

\subsection{Interface utilisateur et pilotage des extractions}

Afin de rendre le fonctionnement du module plus concret, les figures suivantes présentent 
les principales interfaces utilisateur du module \gls{quadrige}, depuis l’exploration des programmes 
jusqu’à l’accès aux résultats d’extraction.


Le frontend Angular du module a été conçu pour accompagner l’utilisateur à travers les 
différentes étapes du processus : définition des filtres, extraction des programmes, sélection 
des données et lancement des extractions. Des contrôles de cohérence sont intégrés afin de 
prévenir les erreurs de paramétrage, notamment sur les champs obligatoires ou les périodes 
temporelles.

Compte tenu du volume potentiellement élevé de programmes retournés par l’extraction 
— pouvant dépasser plusieurs dizaines de résultats selon le périmètre géographique — 
l’interface intègre des mécanismes de recherche par mots-clés afin de faciliter leur exploration 
et leur sélection. Il est ainsi possible de filtrer dynamiquement la liste des programmes extraits 
en saisissant un terme dans la barre de recherche, permettant de cibler rapidement un programme 
par son nom ou ses métadonnées associées.



\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/quadrige/extracted_programs.png}
    \caption{Liste des programmes \gls{quadrige} extraits pour un périmètre géographique donné, tri par mots clés avec la barre de recherche}
    \label{fig:ui-program-list}
\end{figure}



Cette logique de recherche est également appliquée lors des phases de paramétrage. 
Dans les interfaces de filtrage, l’utilisateur peut rechercher les champs disponibles à l’extraction 
en saisissant directement du texte dans le champ dédié, ce qui facilite la sélection lorsque la 
liste des champs est étendue. De la même manière, les localisations suggérées lors de la définition 
des filtres peuvent être recherchées par saisie textuelle, améliorant l’ergonomie et limitant les 
erreurs de sélection.



\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/quadrige/program_filter.png}
    \caption{Interface de définition des filtres d’extraction des programmes \gls{quadrige}}
    \label{fig:ui-program-filter}
\end{figure}







Les résultats des extractions sont présentés sous forme de liens téléchargeables, accompagnés 
d’indicateurs visuels signalant les éventuels avertissements. Cette approche permet à 
l’administrateur de conserver une maîtrise complète sur les données produites avant toute 
intégration dans GeoNature ou dans des plateformes partenaires telles que Borbonica.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/quadrige/extracted_data.png}
    \caption{Résultats des extractions \gls{quadrige} et accès aux fichiers générés}
    \label{fig:ui-extraction-results}
\end{figure}




Une interface spécifique permet également de définir les filtres d’extraction des données 
associées aux programmes sélectionnés (périodes temporelles, localisations, champs à extraire). 
À des fins de lisibilité, cette interface est présentée en annexe (voir Figure~\ref{fig:data-filter}).




\subsection{Synthèse}

L’architecture retenue pour le module \gls{quadrige} répond aux contraintes spécifiques de l’\gls{api} 
Ifremer tout en respectant les principes de modularité, de sécurité et de traçabilité propres à 
GeoNature. Elle constitue une base technique solide pour des évolutions futures, notamment vers 
une automatisation partielle des imports, tout en conservant un contrôle humain sur les données 
produites.

\section{Contraintes, dépendances et livrables du module \gls{quadrige}}

La réussite du projet dépendait principalement de l’accessibilité de l’\gls{api} \gls{quadrige}, de la
stabilité de ses services, et de la disponibilité d’une documentation actualisée. L’intégration dans
GeoNature nécessitait également de respecter la structure modulaire du noyau applicatif et les
contraintes du modèle \gls{sinp}.

Les livrables attendus comprenaient le code source du module, un fichier de configuration, une
documentation à destination des administrateurs (installation, configuration, maintenance) ainsi
qu’un guide d’utilisation orienté métier.

Plusieurs évolutions ont été envisagées : automatisation des imports périodiques, gestion des
imports incrémentaux, intégration à la gestion des droits de GeoNature et, à plus long terme,
publication du module dans le catalogue officiel des extensions GeoNature.



\section{Extension du module \gls{api2gn} pour l’intégration des données Pl@ntNet}

\subsection{Rôle du module \gls{api2gn} dans l’écosystème GeoNature}

Le module \texttt{\gls{api2gn}} constitue une brique transversale au sein de l’écosystème GeoNature. 
Il se distingue des modules d’import classiques, généralement fondés sur des fichiers intermédiaires produits manuellement, 
en permettant l’intégration directe et automatisée de données issues de sources externes.


Ces sources sont exposées via des services web, tels que des \gls{apirest}, des flux JSON structurés ou des services WFS. 
Le module a été conçu pour répondre à des besoins d’automatisation et de récurrence, tout en respectant les contraintes 
du modèle de données du \gls{sinp} et de la table \textit{Synthèse} de GeoNature.

Le code source du module api2GN est disponible publiquement sur GitHub \href{https://github.com/PnX-SI/api2GN}{Dépôt officiel api2GN}.

Son fonctionnement repose sur une séparation claire entre deux niveaux. 
Le premier correspond au cœur du moteur d’import, chargé de l’orchestration des appels aux \gls{api}, 
de la gestion des erreurs, de l’historisation des opérations et de l’insertion des données en base. 
Le second niveau est constitué de parseurs spécialisés, responsables de la transformation des données sources 
vers le modèle attendu par GeoNature.

Chaque parseur implémente ainsi une logique métier propre à une source donnée, tout en s’appuyant sur un socle commun. 
Ce socle prend en charge la gestion des géométries, la résolution des nomenclatures, la validation des mappings, 
l’historisation des imports et l’intégration aux mécanismes de sécurité de GeoNature.

Dans ce contexte, le module \texttt{\gls{api2gn}} apparaît particulièrement adapté à l’intégration de données issues 
de plateformes participatives ou ouvertes, telles que Pl@ntNet. 
Ces plateformes se caractérisent par des volumes importants, une fréquence de mise à jour élevée 
et des modalités d’accès très différentes de celles des bases institutionnelles comme \gls{quadrige}.

\subsection{Objectifs et positionnement du parser Pl@ntNet}

Le fonctionnement de l’API Pl@ntNet, utilisée pour l’extraction des observations, est documenté par la plateforme elle-même\footnote{\href{https://my-api.plantnet.org/}{Documentation de l’API Pl@ntNet}}.


Le développement du parser Pl@ntNet s’inscrit dans une logique complémentaire à celle du module \gls{quadrige}. 
Alors que l’import \gls{quadrige} vise des données institutionnelles, produites dans un cadre scientifique structuré 
et contrôlé, les données issues de Pl@ntNet relèvent d’un contexte participatif.

Ce type de données se caractérise par une production massive et continue d’observations, 
une grande hétérogénéité des contributeurs et une variabilité importante de la qualité des identifications. 
La fiabilité des données repose en grande partie sur des mécanismes automatisés de reconnaissance et de validation.

Dans ce contexte, l’objectif du parser Pl@ntNet n’était pas de proposer une intégration exhaustive 
de l’ensemble des observations disponibles. 
L’enjeu consistait plutôt à mettre en place une chaîne d’import automatisée mais maîtrisée, 
capable de fonctionner régulièrement, tout en conservant un niveau élevé d’exigence sur la qualité des données intégrées.

Le parser a ainsi été conçu comme un outil configurable et sélectif. 
Il permet d’appliquer des filtres spatiaux, temporels et taxonomiques, 
et impose un contrôle strict de la compatibilité des données avec le référentiel \gls{taxref}.

Cette approche illustre une stratégie d’intégration différente de celle retenue pour les données \gls{quadrige}. 
Elle repose sur une automatisation plus poussée, compensée par des mécanismes renforcés de validation et de traçabilité.

\subsection{Architecture technique et principes de conception}

Sur le plan technique, le parser Pl@ntNet repose sur l’architecture standard des parseurs du module \texttt{\gls{api2gn}}. 
Il hérite directement de la classe \texttt{JSONParser}, elle-même dérivée de la classe générique \texttt{Parser}. 
Cette hiérarchie permet de mutualiser l’ensemble des fonctionnalités communes aux sources de données exposées sous forme JSON.

Le parser conserve toutefois la responsabilité de la logique métier spécifique à Pl@ntNet. 
Le déroulement général d’une exécution suit une séquence clairement définie. 
Après l’initialisation du parser et le chargement de la configuration, 
les requêtes vers l’\gls{api} Pl@ntNet sont construites. 
Les observations sont ensuite récupérées de manière paginée, puis transformées et normalisées. 
Une phase de résolution taxonomique est alors appliquée, avant l’insertion des observations valides 
dans la table \textit{Synthèse} de GeoNature.

Un principe fondamental a guidé la conception du parser. 
Aucune logique métier n’est codée en dur dans le code Python. 
Les paramètres d’appel à l’\gls{api}, les filtres appliqués, le mapping des champs 
et les règles de validation sont intégralement définis dans la configuration.

Ce choix favorise la maintenabilité du module et facilite son adaptation à d’autres contextes territoriaux, 
sans remise en cause de l’architecture existante.

\subsection{Configuration dynamique et pilotage par fichier TOML}

L’un des apports majeurs du travail réalisé concerne la gestion de la configuration du parser Pl@ntNet. 
Celui-ci s’appuie sur un fichier \texttt{TOML}, chargé via le mécanisme standard de configuration de GeoNature.

Ce fichier permet de définir de manière centralisée les paramètres d’accès à l’\gls{api} Pl@ntNet, 
les filtres taxonomiques éventuels, les bornes temporelles d’extraction et l’emprise géographique ciblée, 
définie sous la forme d’un polygone GeoJSON. 
Le mapping entre les champs fournis par Pl@ntNet et ceux de la table \textit{Synthèse} 
est également entièrement décrit dans ce fichier.



Le parser implémente un mécanisme de configuration par défaut.

En l’absence de fichier \texttt{api2gn\_config.toml}, ou si celui-ci est incomplet, 
un jeu de valeurs par défaut intégrées au code est utilisé.
Ce comportement permet de garantir le fonctionnement du module en environnement de test 
et de limiter les risques de blocage en production, tout en signalant explicitement 
les incohérences via les logs.

Une route API dédiée permet de consulter la configuration effective chargée par GeoNature.
Cette route applique une validation métier non bloquante, signalant les incohérences potentielles (dates inversées, géométrie invalide, mapping incorrect) sans empêcher l’exécution du parser.
Ce choix vise à favoriser la transparence et l’autonomie des administrateurs, tout en conservant un haut niveau de robustesse.

Le parser Pl@ntNet met également en œuvre un mécanisme d’auto-création des métadonnées nécessaires à l’intégration dans GeoNature.
Lors de la première exécution, le parser vérifie l’existence de la source, du cadre d’acquisition et du jeu de données associés à Pl@ntNet.
En l’absence de ces éléments, ils sont automatiquement créés en base, dans le respect du modèle GeoNature.
Ce mécanisme permet de rendre le parser autonome et de limiter les opérations manuelles d’initialisation lors du déploiement.

La structuration des données repose sur le standard Darwin Core, tel que défini par le \href{https://www.gbif.org/fr/darwin-core}{GBIF}.


Cette stratégie renforce la robustesse du module, notamment en phase de test ou de développement, 
tout en incitant à une configuration explicite en environnement de production. 
Elle ouvre également la voie à une future interface d’administration graphique dédiée 
au pilotage de ces paramètres.



\subsection{Résolution taxonomique et contrôle de la qualité des données}

La résolution taxonomique constitue l’enjeu central de l’intégration des données Pl@ntNet dans GeoNature. 
Afin de garantir la cohérence avec le référentiel national TAXREF et les exigences du \gls{sinp}, 
le parser met en œuvre une chaîne de validation stricte.

Pour chaque observation, le nom scientifique fourni par l’\gls{api} Pl@ntNet est d’abord normalisé. 
Les mentions infra-spécifiques et les annotations non standard sont supprimées afin d’améliorer 
les chances de correspondance avec le référentiel.

Le parser tente ensuite de résoudre le \texttt{cd\_nom} en interrogeant en priorité le référentiel TAXREF local. 
En cas d’échec, le service \gls{taxrefld} du \gls{mnhn} est sollicité. 
Le \texttt{cd\_nom} obtenu est enfin vérifié dans la base locale de GeoNature.

Afin d’optimiser les performances lors des imports volumineux, un cache mémoire des résolutions taxonomiques est utilisé.
Ce cache permet d’éviter des requêtes répétées vers la base TAXREF locale ou les services \gls{taxrefld} pour un même nom scientifique, 
réduisant significativement le temps de traitement.


Par défaut, le parser fonctionne en mode strict. 
Toute observation pour laquelle aucun \texttt{cd\_nom} valide ne peut être résolu est rejetée. 
Ces rejets sont systématiquement journalisés, ce qui permet d’identifier les taxons problématiques 
et d’évaluer la qualité globale des données importées.

\subsection{Traçabilité, sécurité et automatisation des imports}

Le parser Pl@ntNet s’intègre pleinement aux mécanismes de traçabilité fournis par le module \texttt{\gls{api2gn}}. 
Chaque exécution est historisée dans une table dédiée. 
Cette historisation permet de conserver la date du dernier import, 
le nombre d’observations intégrées et le volume total de données importées.

Un mode \textit{dry-run} est systématiquement disponible. 
Il permet de simuler un import sans insertion en base. 
Ce mode constitue un outil essentiel pour tester une nouvelle configuration, 
évaluer l’impact d’un changement de périmètre ou analyser la qualité des données 
avant une intégration effective.

Le module intègre un mécanisme d’automatisation basé sur Celery.
Les parseurs peuvent être configurés avec une fréquence d’exécution stockée en base.
Un processus périodique vérifie les dates de dernier import et déclenche automatiquement les imports lorsque la 
fréquence définie est atteinte. Il devient ainsi possible d’envisager des imports périodiques automatisés, 
tout en conservant un contrôle fin sur leur déclenchement et leur suivi.

\subsection{Apports du travail réalisé}

Le développement du parser Pl@ntNet a permis de démontrer la capacité de GeoNature 
à intégrer des données participatives de manière automatisée, 
tout en respectant les exigences du \gls{sinp} en matière de qualité, de traçabilité 
et de cohérence taxonomique.

Au-delà du cas spécifique de Pl@ntNet, ce travail met en évidence la pertinence 
de l’architecture \texttt{\gls{api2gn}} pour l’intégration de sources de données hétérogènes. 
Il souligne également l’intérêt d’une configuration externalisée 
pour favoriser la réutilisation du module et son adaptation à différents contextes.

Le parser Pl@ntNet constitue ainsi un prototype fonctionnel et robuste. 
Il ouvre la voie à l’intégration d’autres sources de données ouvertes 
et contribue à l’enrichissement progressif de la base GeoNature, 
notamment pour les taxons terrestres peu couverts par les réseaux institutionnels traditionnels.

Le code source du parser Pl@ntNet et des modules développés durant le stage est versionné et documenté, 
afin d’en faciliter la reprise et l’évolution, les références étant fournies en Webographie.
