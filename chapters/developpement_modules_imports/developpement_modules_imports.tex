\section{Contexte du projet}

Le \gls{sinp}, déployé à La Réunion à travers la plateforme Borbonica, constitue le dispositif régional de référence pour la centralisation, 
la gestion et la diffusion des données naturalistes. Depuis sa mise en service, il a permis de structurer et de valoriser un volume important 
d’observations, issues de sources multiples et couvrant un large éventail de taxons.

La dynamique de peuplement de la base reflète toutefois les modalités historiques et opérationnelles de production des données 
naturalistes sur le territoire. Les observations actuellement intégrées concernent majoritairement les milieux terrestres et 
dulçaquicoles, qui bénéficient de réseaux d’acteurs structurés, de protocoles d’acquisition largement diffusés et d’outils de 
saisie directement compatibles avec le SINP.

À l’inverse, les données issues du milieu marin demeurent plus faiblement représentées. Les indicateurs de contenu disponibles 
en 2024 montrent que les observations relatives aux taxons marins ne constituent qu’une part marginale des données référencées. 
Cette situation ne traduit pas une absence de connaissances ou de suivis en milieu marin, mais résulte principalement de la 
complexité des chaînes d’acquisition, de structuration et de diffusion propres aux données marines, historiquement gérées au sein 
de systèmes d’information spécialisés.

Dans ce contexte, l’enjeu principal n’était pas de refondre les outils existants, mais de renforcer progressivement la représentation 
des données marines au sein du \gls{sinp}, en s’appuyant sur des sources de données déjà structurées, pérennes et reconnues 
institutionnellement. Parmi celles-ci, le système d’information \gls{quadrige}, maintenu par l’Ifremer, occupe une place centrale. 
\gls{quadrige} regroupe un volume important de données environnementales et biologiques collectées en milieu marin, issues de programmes 
de surveillance, d’observation et de recherche conduits sur le long terme.

L’ouverture récente d’une \gls{api} GraphQL par l’Ifremer constitue à ce titre une évolution majeure. Elle permet une interrogation 
directe, fine et structurée des données de \gls{quadrige}, en ciblant précisément les programmes, les zones géographiques, les périodes 
temporelles et les types d’observations d’intérêt. Cette évolution ouvre la voie à une exploitation plus systématique de ces données 
par des acteurs institutionnels tels que la DEAL, et à leur mobilisation progressive dans le cadre du \gls{sinp}.

C’est dans ce cadre que s’inscrit le projet développé durant le stage. L’objectif n’était pas de réaliser une intégration 
directe et entièrement automatisée des données \gls{quadrige} dans la base GeoNature, mais de concevoir un module externe permettant 
d’explorer, d’extraire et de préparer ces données de manière structurée, traçable et reproductible. Le module développé interroge 
l’\gls{api} \gls{quadrige} afin d’identifier les programmes pertinents pour le territoire réunionnais, puis permet d’en extraire les observations 
associées selon des critères définis par l’utilisateur.

Les résultats de ces extractions ne prennent pas la forme de fichiers immédiatement importables dans GeoNature. 
Pour chaque programme sélectionné, le module génère une archive compressée contenant plusieurs éléments complémentaires : 
un fichier CSV brut correspondant aux données extraites, un fichier JSON décrivant précisément les filtres appliqués lors 
de l’appel à l’\gls{api}, ainsi qu’un fichier README documentant le déroulement de l’export et les éventuelles anomalies rencontrées. 
Le module conserve également un accès aux trois derniers exports réalisés afin d’en faciliter la consultation et la réutilisation.

En complément de ces archives, le module expose également des liens directs vers les fichiers CSV générés lors de la phase d’extraction 
des programmes. Deux niveaux de fichiers sont distingués :
– un CSV brut issu directement de la réponse de l’\gls{api} \gls{quadrige}, contenant l’ensemble des instances des programmes dont au moins une 
occurrence est localisée sur le territoire ciblé ;
– un CSV filtré géographiquement, obtenu par un traitement a posteriori à l’aide de la bibliothèque pandas, permettant d’exclure 
les instances dont le code de monitoring location ne correspond pas au territoire d’intérêt (par exemple, codes ne commençant pas 
par le préfixe « 126- » pour La Réunion).

Ce filtrage complémentaire est nécessaire car, lors de l’extraction des programmes, la requête adressée à l’\gls{api} \gls{quadrige} sélectionne 
l’ensemble des programmes possédant au moins une instance sur le territoire demandé, mais retourne également les autres instances associées 
à ces programmes, y compris celles situées hors du périmètre géographique ciblé.

Cette approche intermédiaire répond à plusieurs objectifs. Elle garantit d’une part la traçabilité complète des extractions réalisées 
depuis \gls{quadrige}, en conservant une description explicite des paramètres et traitements appliqués. Elle offre d’autre part une souplesse 
d’usage, en laissant aux administrateurs et gestionnaires de données la possibilité de contrôler, d’analyser et, le cas échéant, d’adapter 
les fichiers produits avant toute intégration dans GeoNature ou Borbonica. Le module s’inscrit ainsi comme une brique préparatoire, destinée 
à sécuriser et à faciliter l’intégration future des données marines dans le \gls{sinp}.

Ce développement s’inscrit pleinement dans la dynamique d’amélioration continue portée par la DEAL Réunion et ses partenaires. 
Il vise à renforcer progressivement la place des données issues du milieu marin au sein du système régional, tout en respectant 
les contraintes techniques, méthodologiques et organisationnelles propres aux outils existants.



\section{Périmètre fonctionnel}

Le module d’import devait couvrir l’ensemble de la chaîne d’acquisition : de la découverte des
programmes \gls{quadrige} jusqu’à la production d’un fichier structuré pour GeoNature.  
La première étape consistait à interroger l’\gls{api} en mode authentifié afin d’obtenir la liste des
programmes disponibles pour un utilisateur donné. Un filtrage automatique sur un périmètre
géographique — principalement La Réunion, mais extensible à d’autres territoires comme les Îles
Éparses — permettait d’isoler les programmes pertinents. Une interface dédiée intégrée à
GeoNature offrait ensuite la possibilité de rechercher des programmes, d’affiner l’affichage par
mots-clés et de sélectionner ceux à importer.

Une fois les programmes choisis, l’utilisateur pouvait définir les filtres à appliquer aux données
(la période d'intérêt, les champs souhaités, ou encore la reprise des stations déjà extraites).  
Le module interrogeait alors \href{https://\gls{quadrige}-core.ifremer.fr/\gls{api}/extraction/doc?doc=standard&lang=fr&name=result&type=standard}{l’\gls{api} \gls{quadrige}}
pour récupérer les observations correspondantes \href{https://\gls{quadrige}.ifremer.fr/support/Mes-donnees/J-extrais-mes-donnees/J-interroge-l-\gls{api}-pour-extraire-mes-donnees/Je-regarde-des-videos-de-demo-sur-l-\gls{api}}{Tutoriels vidéo}.
Seuls les champs utiles au modèle \gls{sinp} étaient extraits : identifiants des programmes et stations,
localisation géographique, taxon observé, date, ainsi que les métadonnées essentielles (auteur,
organisme, méthode d’acquisition, etc.). Une transformation était appliquée pour obtenir une
structure compatible avec les mécanismes d’import de GeoNature.

Enfin, le module produisait un fichier CSV intermédiaire, destiné à être importé via
l’infrastructure existante de GeoNature. Chaque opération d’import était consignée dans un
historique affiché dans un second onglet, permettant de suivre les actions réalisées, leur date,
leur statut et les éventuelles erreurs rencontrées. L’ensemble du module était réservé aux
administrateurs, conformément aux pratiques habituelles de contrôle des imports dans GeoNature.

\section{Conception technique du module d’import Quadrige}

Le module Quadrige développé durant le stage a été conçu comme un module externe à GeoNature, 
respectant l’architecture recommandée par le projet tout en prenant en compte les contraintes 
spécifiques de l’API GraphQL mise à disposition par l’Ifremer. L’objectif principal était de 
proposer une chaîne d’extraction robuste, traçable et exploitable, sans perturber le cœur 
applicatif de GeoNature ni les processus existants de gestion des données naturalistes.

\subsection{Architecture générale et découplage frontend/backend}

Le module repose sur une architecture client--serveur classique, intégrée à GeoNature via un 
\textit{blueprint} Flask côté backend et un module Angular dédié côté frontend.  
Le backend est responsable de l’ensemble des interactions avec l’API Quadrige, incluant 
l’authentification, la construction des requêtes GraphQL, le suivi des extractions, la gestion 
des fichiers produits et l’exposition de routes REST sécurisées.

Le frontend se limite volontairement au pilotage des opérations : sélection des paramètres, 
lancement des extractions et visualisation des résultats. Il n’accède jamais directement à 
l’API Quadrige ni aux paramètres sensibles, ce qui permet de renforcer la sécurité globale du 
dispositif.

Ce découplage permet :
\begin{itemize}
  \item de centraliser les accès à l’API Quadrige et les jetons d’authentification ;
  \item de limiter la surface d’exposition des données sensibles ;
  \item de faciliter la maintenance et les évolutions indépendantes du backend et du frontend.
\end{itemize}

\subsection{Gestion de la configuration et des paramètres sensibles}

Le module s’appuie sur un fichier de configuration dédié, chargé côté backend via le mécanisme 
standard de GeoNature (\texttt{geonature/config}). Ce fichier centralise l’ensemble des paramètres 
techniques nécessaires au fonctionnement du module, notamment :
\begin{itemize}
  \item l’URL de l’API GraphQL Quadrige ;
  \item le jeton d’authentification requis pour les appels à l’API ;
  \item des paramètres métiers optionnels, tels que les localisations suggérées ou les champs 
  extractibles.
\end{itemize}

Une route backend permet d’exposer cette configuration au frontend de manière contrôlée, afin 
de rendre l’interface dynamique et adaptable. Certaines données métiers restent actuellement 
définies côté frontend, mais l’architecture mise en place permettrait de les centraliser 
entièrement côté backend dans une version ultérieure, sans remise en cause du fonctionnement 
global.

\subsection{Extraction des programmes Quadrige}

La première étape du processus consiste à identifier les programmes Quadrige pertinents pour un 
territoire donné. Cette extraction repose sur une requête GraphQL spécifique fournie par l’API 
Quadrige, filtrée à partir d’un préfixe de \textit{monitoring location} correspondant au périmètre 
géographique ciblé.

L’API Quadrige retournant l’ensemble des instances associées à un programme dès lors qu’au moins 
une station correspond au critère de recherche, un traitement complémentaire est appliqué côté 
backend. Les fichiers CSV bruts générés par l’API sont filtrés a posteriori à l’aide de la 
bibliothèque \texttt{pandas}, afin de ne conserver que les programmes effectivement localisés sur 
le territoire demandé.

Cette étape permet de produire :
\begin{itemize}
  \item un CSV brut, conservé à des fins de traçabilité et de vérification ;
  \item un CSV filtré, utilisé pour l’affichage et la sélection des programmes dans le frontend.
\end{itemize}

Les métadonnées associées à chaque extraction (localisation, horodatage, filtre utilisé) sont 
sauvegardées afin de permettre la reprise des extractions et la consultation des résultats 
antérieurs.

\subsection{Extraction des données et gestion asynchrone des traitements}

L’extraction des données repose sur un mécanisme asynchrone propre à l’API Quadrige. Pour chaque 
programme sélectionné, une requête GraphQL est soumise afin de lancer un job d’extraction côté 
serveur Ifremer. Le module implémente un mécanisme de \textit{polling} permettant de suivre 
l’évolution de l’état de chaque extraction (PENDING, RUNNING, SUCCESS, WARNING, ERROR).

Les extractions sont gérées de manière indépendante, ce qui permet :
\begin{itemize}
  \item de paralléliser les traitements sur plusieurs programmes ;
  \item de gérer finement les échecs ou avertissements retournés par l’API ;
  \item d’éviter un blocage global en cas d’échec partiel.
\end{itemize}

À l’issue des traitements, les fichiers ZIP générés par l’API Quadrige sont téléchargés et stockés 
temporairement sur le serveur GeoNature. Chaque fichier est renommé selon une convention 
explicite intégrant le programme, la localisation et l’horodatage, garantissant ainsi sa 
traçabilité.

\subsection{Gestion des états, des erreurs et de la traçabilité}

Un soin particulier a été apporté à la gestion des états et des erreurs tout au long du processus 
d’extraction. Les situations suivantes sont explicitement prises en compte :
\begin{itemize}
  \item erreurs de communication avec l’API GraphQL ;
  \item délais excessifs lors des extractions longues ;
  \item incohérences ou réponses partielles retournées par l’API ;
  \item extractions aboutissant à des avertissements ou à l’absence de données.
\end{itemize}

Chaque extraction génère un retour structuré indiquant son statut, les avertissements éventuels 
et les liens vers les fichiers produits. Ces informations sont transmises au frontend afin de 
permettre à l’utilisateur d’identifier rapidement les extractions exploitables et celles 
nécessitant une vérification.

\subsection{Interface utilisateur et pilotage des extractions}

Afin de rendre le fonctionnement du module plus concret, les figures suivantes présentent 
les principales interfaces utilisateur du module Quadrige, depuis l’exploration des programmes 
jusqu’à l’accès aux résultats d’extraction.


Le frontend Angular du module a été conçu pour accompagner l’utilisateur à travers les 
différentes étapes du processus : définition des filtres, extraction des programmes, sélection 
des données et lancement des extractions. Des contrôles de cohérence sont intégrés afin de 
prévenir les erreurs de paramétrage, notamment sur les champs obligatoires ou les périodes 
temporelles.

Compte tenu du volume potentiellement élevé de programmes retournés par l’extraction 
— pouvant dépasser plusieurs dizaines de résultats selon le périmètre géographique — 
l’interface intègre des mécanismes de recherche par mots-clés afin de faciliter leur exploration 
et leur sélection. Il est ainsi possible de filtrer dynamiquement la liste des programmes extraits 
en saisissant un terme dans la barre de recherche, permettant de cibler rapidement un programme 
par son nom ou ses métadonnées associées.

Cette logique de recherche est également appliquée lors des phases de paramétrage. 
Dans les interfaces de filtrage, l’utilisateur peut rechercher les champs disponibles à l’extraction 
en saisissant directement du texte dans le champ dédié, ce qui facilite la sélection lorsque la 
liste des champs est étendue. De la même manière, les localisations suggérées lors de la définition 
des filtres peuvent être recherchées par saisie textuelle, améliorant l’ergonomie et limitant les 
erreurs de sélection.



\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/quadrige/program_filter.png}
    \caption{Interface de définition des filtres d’extraction des programmes Quadrige}
    \label{fig:ui-program-filter}
\end{figure}



\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/quadrige/extracted_programs.png}
    \caption{Liste des programmes Quadrige extraits pour un périmètre géographique donné, tri par mots clés avec la barre de recherche}
    \label{fig:ui-program-list}
\end{figure}



Les résultats des extractions sont présentés sous forme de liens téléchargeables, accompagnés 
d’indicateurs visuels signalant les éventuels avertissements. Cette approche permet à 
l’administrateur de conserver une maîtrise complète sur les données produites avant toute 
intégration dans GeoNature ou dans des plateformes partenaires telles que Borbonica.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/quadrige/extracted_data.png}
    \caption{Résultats des extractions Quadrige et accès aux fichiers générés}
    \label{fig:ui-extraction-results}
\end{figure}




Une interface spécifique permet également de définir les filtres d’extraction des données 
associées aux programmes sélectionnés (périodes temporelles, localisations, champs à extraire). 
À des fins de lisibilité, cette interface est présentée en annexe (voir Figure~\ref{fig:data-filter}).




\subsection{Synthèse}

L’architecture retenue pour le module Quadrige répond aux contraintes spécifiques de l’API 
Ifremer tout en respectant les principes de modularité, de sécurité et de traçabilité propres à 
GeoNature. Elle constitue une base technique solide pour des évolutions futures, notamment vers 
une automatisation partielle des imports, tout en conservant un contrôle humain sur les données 
produites.

\section{Contraintes, dépendances et livrables du module Quadrige}

La réussite du projet dépendait principalement de l’accessibilité de l’\gls{api} \gls{quadrige}, de la
stabilité de ses services, et de la disponibilité d’une documentation actualisée. L’intégration dans
GeoNature nécessitait également de respecter la structure modulaire du noyau applicatif et les
contraintes du modèle \gls{sinp}.

Les livrables attendus comprenaient le code source du module, un fichier de configuration, une
documentation à destination des administrateurs (installation, configuration, maintenance) ainsi
qu’un guide d’utilisation orienté métier.

Plusieurs évolutions ont été envisagées : automatisation des imports périodiques, gestion des
imports incrémentaux, intégration à la gestion des droits de GeoNature et, à plus long terme,
publication du module dans le catalogue officiel des extensions GeoNature.



\section{Extension du module api2GN pour l’intégration des données Pl@ntNet}

\subsection{Rôle du module api2GN dans l’écosystème GeoNature}
Le module \texttt{api2GN} constitue une brique transversale au sein de l’écosystème GeoNature.
Contrairement aux modules d’import classiques, fondés sur des fichiers intermédiaires produits manuellement, 
\texttt{api2GN} a été conçu pour permettre l’intégration directe et automatisée de données issues de sources externes, 
exposées via des services web (API REST, flux JSON, services WFS).

Son principe repose sur une séparation claire entre :
\begin{itemize}
\item le \textbf{cœur du moteur d’import}, chargé de l’orchestration des appels API, de la gestion des erreurs, de 
l’historisation et de l’insertion en base ;
\item des \textbf{parseurs spécialisés}, responsables de la transformation des données sources vers le modèle de la table 
\textit{Synthèse} de GeoNature.
\end{itemize}

Chaque parseur implémente ainsi une logique métier propre à une source de données donnée, tout en 
s’appuyant sur un socle commun fourni par \texttt{api2GN} : gestion des géométries, résolution des nomenclatures, validation du 
mapping, historisation des imports et intégration dans les mécanismes de sécurité de GeoNature.

Dans ce cadre, le module \texttt{api2GN} apparaît comme un outil particulièrement adapté à l’intégration de données issues de 
plateformes participatives ou ouvertes, telles que Pl@ntNet, dont les volumes, la fréquence de mise à jour et les modalités d’accès 
se distinguent fortement des bases institutionnelles comme Quadrige.

\subsection{Objectifs et positionnement du parser Pl@ntNet}

Le développement du parser Pl@ntNet s’inscrit dans une logique complémentaire à celle du module Quadrige.
Alors que l’import Quadrige vise des données institutionnelles, fortement structurées et produites dans un 
cadre scientifique contrôlé, les données Pl@ntNet relèvent d’un contexte participatif, caractérisé par :
\begin{itemize}
\item une production massive et continue d’observations ;
\item une grande hétérogénéité des contributeurs ;
\item une qualité variable des identifications taxonomiques ;
\item une forte dépendance à des mécanismes de validation automatisés.
\end{itemize}

L’objectif du parser Pl@ntNet n’était donc pas de proposer une intégration exhaustive et aveugle de l’ensemble 
des observations disponibles, mais de mettre en place une chaîne d’import :
\begin{itemize}
\item \textbf{automatisée}, afin de pouvoir être exécutée régulièrement ;
\item \textbf{configurable}, pour adapter le périmètre d’import aux besoins territoriaux ;
\item \textbf{sélective}, en appliquant des filtres taxonomiques, spatiaux et temporels ;
\item \textbf{rigoureuse}, en imposant un contrôle strict de la compatibilité avec le référentiel TAXREF.
\end{itemize}

Le parser Pl@ntNet a ainsi été conçu comme un cas d’étude illustrant une stratégie d’intégration différente de 
celle retenue pour Quadrige : une automatisation poussée, compensée par des mécanismes renforcés de validation et de traçabilité.



\subsection{Architecture technique et principes de conception}


Sur le plan technique, le parser Pl@ntNet repose sur l’architecture standard des parseurs \texttt{api2GN}.
Il hérite directement de la classe \texttt{JSONParser}, elle-même dérivée de la classe générique \texttt{Parser}. 
Cette hiérarchie permet de mutualiser les fonctionnalités communes à l’ensemble des sources JSON, tout en laissant 
au parser la responsabilité de la logique métier spécifique à Pl@ntNet.

Le flux général d’exécution peut être résumé ainsi :
\begin{enumerate}
\item initialisation du parser et chargement de la configuration ;
\item construction des requêtes vers l’API Pl@ntNet ;
\item récupération paginée des occurrences ;
\item transformation et normalisation des données ;
\item résolution taxonomique ;
\item insertion des observations valides dans la table \textit{Synthèse}.
\end{enumerate}

Le parser a été conçu selon un principe fondamental : \textbf{aucune logique métier n’est codée en dur}.
Les paramètres d’appel à l’API, les filtres appliqués, le mapping des champs et les règles de validation sont intégralement 
pilotés par la configuration, ce qui permet d’adapter le comportement du parser sans modifier le code Python.

Cette approche favorise la maintenabilité du module et facilite son appropriation par d’autres structures souhaitant exploiter 
les données Pl@ntNet sur des territoires ou des périmètres différents.



\subsection{Configuration dynamique et pilotage par fichier TOML}



L’un des apports majeurs du travail réalisé réside dans la mise en place d’un système de configuration 
entièrement externalisé, basé sur un fichier \texttt{TOML} chargé via le mécanisme standard de configuration de GeoNature.

Ce fichier permet de définir de manière centralisée :
\begin{itemize}
\item les paramètres d’accès à l’API Pl@ntNet (URL, clé API) ;
\item les filtres taxonomiques (liste d’espèces ciblées ou absence de filtrage) ;
\item les bornes temporelles d’extraction ;
\item l’emprise géographique, définie par un polygone GeoJSON ;
\item les règles de mapping entre les champs Pl@ntNet et la table \textit{Synthèse}.
\end{itemize}

Le parser implémente un mécanisme de \textit{fallback} permettant de fonctionner même en l’absence 
de configuration complète.
Dans ce cas, des valeurs par défaut sont appliquées, accompagnées de messages d’avertissement explicites 
dans les logs. Cette stratégie garantit la robustesse du module en environnement de test ou de développement, 
tout en incitant à une configuration explicite en production.

Ce choix de conception permet également d’envisager, à terme, une interface d’administration graphique dédiée à 
la gestion de ces paramètres, sans remise en cause de l’architecture existante.



\subsection{Résolution taxonomique et contrôle de la qualité des données}


La résolution taxonomique constitue l’enjeu central de l’intégration des données Pl@ntNet dans GeoNature.
Afin de garantir la cohérence avec le référentiel national TAXREF et les exigences du SINP, le parser met en œuvre une chaîne de validation stricte.

Pour chaque observation, le nom scientifique fourni par l’API Pl@ntNet est d’abord normalisé afin de supprimer les mentions infra-spécifiques ou les annotations non standard. Le parser tente ensuite de résoudre le \texttt{cd_nom} selon un ordre hiérarchique précis :
\begin{enumerate}
\item recherche dans le référentiel TAXREF local de GeoNature ;
\item en cas d’échec, interrogation du service TAXREF-LD du MNHN ;
\item vérification de l’existence du \texttt{cd_nom} obtenu dans la base locale.
\end{enumerate}

Un cache mémoire est utilisé afin d’éviter les résolutions répétées pour un même taxon, ce qui améliore significativement les performances lors des imports volumineux.

Le parser fonctionne par défaut en \textbf{mode strict}.
Dans ce mode, toute observation pour laquelle aucun \texttt{cd_nom} valide ne peut être résolu est rejetée. Ces rejets sont explicitement journalisés, permettant :
\begin{itemize}
\item d’identifier les taxons problématiques ;
\item d’évaluer la qualité globale des données importées ;
\item d’envisager, le cas échéant, une mise à jour du référentiel TAXREF local.
\end{itemize}

Ce mécanisme constitue un compromis entre automatisation et exigence de qualité, indispensable dans le contexte des données participatives.


\subsection{Traçabilité, sécurité et automatisation des imports}

Le parser Pl@ntNet s’intègre pleinement aux mécanismes de traçabilité fournis par \texttt{api2GN}.
Chaque exécution est historisée dans une table dédiée, permettant de conserver :
\begin{itemize}
\item la date du dernier import ;
\item le nombre d’observations importées lors de la dernière exécution ;
\item le volume total d’observations intégrées ;
\item une fréquence d’exécution planifiée, le cas échéant.
\end{itemize}

Un mode \textit{dry-run} est systématiquement disponible. Il permet de simuler un import sans insertion en base, ce qui constitue un outil essentiel pour :
\begin{itemize}
\item tester une nouvelle configuration ;
\item évaluer l’impact d’un changement de périmètre ;
\item analyser la qualité des données avant intégration effective.
\end{itemize}

Enfin, le module est compatible avec le système de tâches planifiées de GeoNature, permettant d’envisager des imports périodiques automatisés, tout en conservant un contrôle fin sur leur déclenchement et leur suivi.



\subsection{Apports du travail réalisé}


Le développement du parser Pl@ntNet a permis de démontrer la capacité de GeoNature à intégrer des données participatives de manière automatisée, tout en respectant les exigences du SINP en matière de qualité, de traçabilité et de cohérence taxonomique.

Au-delà du cas spécifique de Pl@ntNet, ce travail met en évidence :
\begin{itemize}
\item la pertinence de l’architecture \texttt{api2GN} pour l’intégration de sources hétérogènes ;
\item l’intérêt d’une configuration externalisée pour favoriser la réutilisation et l’adaptabilité ;
\item la complémentarité des stratégies d’import entre données institutionnelles et participatives.
\end{itemize}

Le parser Pl@ntNet constitue ainsi un prototype fonctionnel et robuste, susceptible d’être étendu à d’autres sources de données ouvertes, et de contribuer à l’enrichissement progressif de la base GeoNature, notamment pour les taxons terrestres peu couverts par les réseaux institutionnels traditionnels.